from __future__ import annotations
import argparse
import random
import time
from pathlib import Path
from typing import Optional

import os

from rich.progress import Progress

from utils import Crawler, log


def main() -> str | None:
    # Returns the absolute path of the saved file
    parser = argparse.ArgumentParser(
        description="ğŸ•¸ï¸ åŸºäºDrissionPageåº“çš„èµ·ç‚¹å°è¯´çˆ¬è™«ã€‚"
    )
    parser.add_argument(
        "-m",
        "--mode",
        choices=["full", "range", "word_count"],
        required=True,
        help='ä¸‹è½½æ¨¡å¼ï¼šé€‰æ‹© "full" ä¸ºå…¨æ–‡ä¸‹è½½ï¼Œé€‰æ‹© "range" ä¸ºèŒƒå›´ä¸‹è½½',
    )
    parser.add_argument("url", type=str, help="ç›®å½•é¡µçš„URL")

    # è¿™äº›å‚æ•°ä»…åœ¨'range'æ¨¡å¼ä¸‹éœ€è¦
    parser.add_argument(
        "-u",
        "--upper-bound",
        type=int,
        default=None,
        help='èŒƒå›´ä¸‹è½½çš„ä¸Šç•Œï¼ˆä»…å½“é€‰æ‹© "range" æ¨¡å¼æ—¶æœ‰æ•ˆï¼‰ï¼›æˆ–æ˜¯å­—æ•°ä¸‹è½½çš„æœ€å¤§å­—æ•°ï¼ˆ"word_count"æ¨¡å¼ï¼‰',
    )
    parser.add_argument(
        "-l",
        "--lower-bound",
        type=int,
        default=None,
        help='èŒƒå›´ä¸‹è½½çš„ä¸‹ç•Œï¼ˆä»…å½“é€‰æ‹© "range" æ¨¡å¼æ—¶æœ‰æ•ˆï¼‰',
    )
    args = parser.parse_args()

    if args.mode == "full":
        path = full_download(args.url)
    elif args.mode == "range":
        if args.upper_bound is None or args.lower_bound is None:
            parser.error(
                "åœ¨èŒƒå›´æ¨¡å¼ä¸‹ï¼Œå¿…é¡»åŒæ—¶æä¾› --upper-bound å’Œ --lower-bound"
            )
        path = range_download(args.url, args.lower_bound, args.upper_bound)
    else:
        if args.upper_bound is None:
            parser.error("åœ¨å­—æ•°æ¨¡å¼ä¸‹ï¼Œå¿…é¡»æä¾› --upper-bound")
        path = full_download(args.url, word_limit=args.upper_bound)
    return path


def save(name: str, content: str) -> str:
    # Returns the absolute path of the saved file
    if not os.path.exists("qd_novels"):
        os.mkdir("qd_novels")
    path = Path(f"qd_novels/{name}.txt")
    path.write_text(content, "utf-8")
    return path.resolve().as_posix()


def full_download(url: str, word_limit: Optional[int] = None) -> str | None:
    # Returns the absolute path of the saved file
    crawler = Crawler()
    log.info("ğŸ‰ DrissionPageåˆå§‹åŒ–å®Œæ¯•")

    index = crawler.get_index(url)
    log.info(f"ğŸˆ æ­£åœ¨ä¸‹è½½ã€Š{index.name}ã€‹ï¼Œå…·æœ‰{len(index.chpts)}ç« èŠ‚çš„å°è¯´")

    chpts: list[str] = []
    word_count: int = 0
    with Progress() as progress:
        if word_limit is not None:
            download = progress.add_task("ğŸ›» ä¸‹è½½ä¸­", total=word_limit)
        else:
            download = progress.add_task("ğŸ›» ä¸‹è½½ä¸­", total=len(index.chpts))
        try:
            for info in index.chpts:
                chpt: str = crawler.get_chpt(info.url)
                # Count the number of words in the chapter
                if word_limit is not None:
                    progress.advance(download, advance=min(len(chpt), word_limit - word_count))
                    if word_count > word_limit:
                        break
                else:
                    progress.advance(download)
                word_count += len(chpt)
                chpts.append(chpt)
                # time.sleep(random.uniform(2, 5))
        except Exception as e:
            log.error(e)
        finally:
            content = "\n".join(chpts)
            save_name = index.name
            if word_limit is not None:
                save_name += f"-wl{word_limit}"
            path = save(save_name, content)
            log.info("âœ¨ å°è¯´ä¿å­˜å®Œæ¯•")
            return path


def range_download(url: str, lower_bound: int, upper_bound: int) -> str | None:
    if lower_bound > upper_bound:
        lower_bound, upper_bound = upper_bound, lower_bound
    lower_bound -= 1  # æ›´åŠ ç¬¦åˆä¹ æƒ¯ç”¨æ³•

    crawler = Crawler()
    log.info("ğŸ‰ DrissionPageåˆå§‹åŒ–å®Œæ¯•")

    index = crawler.get_index(url)
    lower_name = index.chpts[lower_bound].name
    upper_name = index.chpts[upper_bound - 1].name
    log.info(
        f"ğŸˆ æ­£åœ¨ä¸‹è½½ã€Š{index.name}ã€‹ï¼ŒèŒƒå›´ä»ã€Š{lower_name}ã€‹åˆ°ã€Š{upper_name}ã€‹"
    )

    chpts: list[str] = []
    with Progress() as progress:
        download = progress.add_task(
            "ğŸ›» ä¸‹è½½ä¸­", total=upper_bound - lower_bound
        )
        try:
            for info in index.chpts[lower_bound:upper_bound]:
                chpt = crawler.get_chpt(info.url)
                chpts.append(chpt)
                progress.advance(download)
                # time.sleep(random.uniform(5, 7))
        except Exception as e:
            log.error(e)
        finally:
            content = "\n".join(chpts)
            path = save(
                f"{index.name}-{lower_bound + 1}-{upper_bound}", content
            )
            log.info("âœ¨ å°è¯´ä¿å­˜å®Œæ¯•")
            return path


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log.error(e)
